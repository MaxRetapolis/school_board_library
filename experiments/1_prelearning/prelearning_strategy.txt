# Strategy for Document Processing and Knowledge Extraction

## Overview

The goal is to process a collection of school board documents to extract valuable information and build a knowledge graph. The documents vary in format and content, including agendas, meeting minutes, and transcripts. This strategy outlines the steps to process these documents, extract entities and relationships, and construct a knowledge graph for further analysis.

## Steps

### 1. Document Ingestion

- **Objective**: Read documents from `raw_documents` and prepare them for processing.
- **Actions**:
  - Identify and list all document files in the `raw_documents` folder.
  - Support various formats (e.g., PDF, DOCX, TXT) by converting them to plain text.
  - **Specific Tools**: Consider using libraries like `PyPDF2` or `pdfminer.six` for PDF conversion, and `python-docx` for DOCX files.
  - **Error Handling**: Implement error handling for files that cannot be read or converted. Log these errors and continue processing other files.

### 2. Text Preprocessing

- **Objective**: Clean and normalize text data for better extraction results.
- **Actions**:
  - Remove unnecessary whitespace, special characters, and formatting.
  - Normalize text (e.g., lowercasing, removing stop words if necessary).
  - **Error Handling**: Handle any unexpected characters or encoding issues that may arise during preprocessing.

### 3. Entity Extraction

- **Objective**: Identify and extract key entities from the text.
- **Entities to Extract**:
  - **Participants**: Names and roles (e.g., board members, teachers, parents).
  - **Events**: Occurrences such as presentations and resolutions.
  - **Topics**: Subjects discussed during meetings.

- **Actions**:
  - Use NLP libraries (e.g., `spaCy`) to perform Named Entity Recognition (NER).
  - Create a list of entities with their types and attributes.
  - **Error Handling**:  Handle cases where NER fails to identify entities, potentially using fallback methods or manual annotation.

### 4. Relationship Extraction

- **Objective**: Determine relationships between entities.
- **Actions**:
  - Analyze sentences to find connections between participants, events, and topics.
  - Use dependency parsing and pattern matching to identify relationships.
  - **Error Handling**: Address cases where relationships are ambiguous or cannot be determined with high confidence.

### 5. Data Structuring

- **Objective**: Organize extracted data into structured formats.
- **Actions**:
  - Define data classes (`BoardMeeting`, `Participant`, `Event`, `Topic`).
    -   **`BoardMeeting` Attributes**: `date`, `location`, `attendees` (list of Participants), `topics` (list of Topics), `resolutions` (list of Events), `meeting_type` (e.g., "Regular", "Special").
    -   **`Participant` Attributes**: `name`, `role` (e.g., "Board Member", "Superintendent", "Teacher", "Parent"), `affiliation`.
    -   **`Event` Attributes**: `event_type` (e.g., "Presentation", "Resolution", "Discussion"), `description`, `outcome`, `related_topics` (list of Topics).
    -   **`Topic` Attributes**: `name`, `description`, `keywords`, `category`.
  - Populate instances of these classes with extracted data.
  - Serialize structured data to JSON or another suitable format.

### 6. Knowledge Graph Construction

- **Objective**: Build a knowledge graph from structured data.
- **Actions**:
  - Use a graph database (e.g., Neo4j) to store entities and relationships.
  - Define node and relationship types consistent with the data classes.
  - Populate the graph database with the structured data.
  - **Knowledge Graph Update Strategy**:  The `run()` function in `step5_update_knowledge_graph.py` should *append* new data to the knowledge graph.  If a meeting with the same identifier (e.g., date and meeting type) already exists, it should update the existing nodes and relationships rather than creating duplicates.  Consider adding a `last_updated` timestamp to the `BoardMeeting` node.

### 7. Iterative Refinement

- **Objective**: Continuously improve the extraction and processing pipeline.
- **Actions**:
  - Evaluate the accuracy of entity and relationship extraction.
  - **Evaluation Metrics**: Use metrics like precision, recall, and F1-score to evaluate the performance of entity and relationship extraction.  Consider using a small, manually annotated dataset for evaluation.
  - Refine preprocessing and extraction methods based on observed results.
  - Update the pipeline to handle edge cases and improve performance.

## Implementation Plan

### Step 1: Document Ingestion Script

- **File**: `src/step1_read_documents.py`
- **Function**: `run()`
- **Description**:
  - Read documents from `raw_documents`.
  - Convert documents to plain text.
  - Save converted text files to `processed_documents`.

### Step 2: Text Preprocessing Script

- **File**: `src/step2_preprocess_documents.py`
- **Function**: `run()`
- **Description**:
  - Clean and normalize the text from `processed_documents`.
  - Save preprocessed text to `data/processed_documents_clean`.

### Step 3: Entity Extraction Script

- **File**: `src/step3_extract_entities.py`
- **Function**: `run()`
- **Description**:
  - Extract entities from preprocessed text.
  - Save entities to `data/entities` as JSON files.

### Step 4: Relationship Extraction Script

- **File**: `src/step4_extract_relationships.py`
- **Function**: `run()`
- **Description**:
  - Analyze sentences to extract relationships between entities.
  - Save relationships to `data/relationships` as JSON files.

### Step 5: Data Structuring and Knowledge Graph Update

- **File**: `src/step5_update_knowledge_graph.py`
- **Function**: `run()`
- **Description**:
  - Structure the extracted data using defined classes.
  - Update the Neo4j knowledge graph with entities and relationships.

### Utilities and Configuration

- **Utilities**:
  - **File**: `src/utils/helper_functions.py`
  - **Description**: Common functions used across multiple scripts (e.g., loading configurations).

- **Configuration**:
  - **File**: `configs/config.yaml`
  - **Description**: Contains configuration settings like file paths and database credentials.
  - **Expected Parameters**:
    -   `raw_documents_dir`: Path to the directory containing raw documents.
    -   `processed_documents_dir`: Path to the directory for storing processed (plain text) documents.
    -   `clean_documents_dir`: Path to store cleaned, preprocessed text.
    -   `entities_dir`: Path to store extracted entities.
    -   `relationships_dir`: Path to store extracted relationships.
    -   `neo4j_uri`: Connection URI for the Neo4j database.
    -   `neo4j_user`: Username for the Neo4j database.
    -   `neo4j_password`: Password for the Neo4j database.
    -   `spacy_model`: Name of the spaCy model to use (e.g., "en_core_web_sm").

## Next Steps

- **Develop Individual Scripts**: Implement the logic for each script based on the outlined objectives.
- **Define Data Classes**: Create Python classes for `BoardMeeting`, `Participant`, `Event`, and `Topic` with the attributes specified above.
- **Set Up Knowledge Graph**: Install and configure Neo4j, and ensure connectivity from the script.
- **Testing**: Test each step individually before running the entire pipeline.
- **Documentation**: Document any assumptions, decisions, and guidelines within the code and supporting files.

---
